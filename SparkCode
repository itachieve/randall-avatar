import pyspark
import sys

# ==============================================================================
# --- This part of the code runs on the Spark Driver ---
# ==============================================================================

print("\n--- Testing from the Spark Driver (Master Node) ---")
print(f"Driver's sys.path: {sys.path}")

# Test: Can we import the spacy core library?
# This should succeed because `--py-files` adds the zip to the PYTHONPATH for both driver and executors.
try:
    import spacy
    print("âœ… SUCCESS: spaCy module was successfully imported on the driver.")
    spacy_version = spacy.__version__
    print(f"   -> spaCy version: {spacy_version}")
except ImportError as e:
    print(f"âŒ FAIL: spaCy module could NOT be imported on the driver. Error: {e}")


# ==============================================================================
# --- This part of the code runs on the Spark Executor(s) ---
# ==============================================================================

def test_on_executor(x):
    """
    This function is sent to and executed on the worker nodes (executors).
    """
    # Test: Can we import the spaCy core library on the executor?
    try:
        import spacy
        spacy_import_status = "SUCCESS"
        spacy_import_error = None
    except ImportError as e:
        spacy_import_status = "FAIL"
        spacy_import_error = str(e)
    
    return (
        f"spaCy Import Status: {spacy_import_status}",
        f"spaCy Import Error: {spacy_import_error}"
    )

if __name__ == "__main__":
    spark = pyspark.sql.SparkSession.builder.appName("SpacyImportTest").getOrCreate()
    sc = spark.sparkContext
    
    # Create an RDD to run the test function on the executors.
    test_rdd = sc.parallelize([1])
    
    print("\n--- Starting Spark Task on Executor(s) ---")
    
    # Collect the results from all executors
    results = test_rdd.map(test_on_executor).collect()
    
    print("\n--- Results from Executor(s) ---")
    for result in results:
        for line in result:
            print(f"   -> {line}")

    spark.stop()
