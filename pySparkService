from __future__ import annotations

import datetime
import shlex
from typing import Optional, Sequence, Union, Dict, List

from airflow.providers.google.cloud.hooks.dataproc import DataprocHook, DataProcJobBuilder
# Airflow is expected to provide logging via task logger; fall back to stdlib if needed
try:
    from airflow.utils.log.logging_mixin import LoggingMixin
except Exception:  # pragma: no cover
    class LoggingMixin:
        import logging
        log = logging.getLogger(__name__)


class SparkService(LoggingMixin):
    """
    Thin service around DataprocHook + DataProcJobBuilder to submit & monitor PySpark jobs.

    Key ideas:
    - Build the payload with DataProcJobBuilder (exact same fields that the Dataproc API expects).
    - Submit with DataprocHook.submit_job(...).
    - Optionally wait for completion with DataprocHook.wait_for_job(...).
    - Log a faithful, reconstructed `gcloud dataproc jobs submit pyspark` command for debugging.
    """

    def __init__(
        self,
        project_id: str,
        region: str,
        gcp_conn_id: str = "google_cloud_default",
        impersonation_chain: Optional[Union[str, Sequence[str]]] = None,
        delegate_to: Optional[str] = None,  # Ignored by Hook in newer providers; kept for compatibility knobs
    ):
        self.project_id = project_id
        self.region = region
        self.gcp_conn_id = gcp_conn_id
        self.impersonation_chain = impersonation_chain
        self.delegate_to = delegate_to

        self._hook = DataprocHook(
            gcp_conn_id=self.gcp_conn_id,
            impersonation_chain=self.impersonation_chain,
            # `delegate_to` is legacy; not all provider versions support it at init. Omit unless you must.
        )

    # -------------------------
    # Job building
    # -------------------------
    def build_pyspark_job(
        self,
        *,
        task_id: str,
        cluster_name: str,
        main: str,                            # gs://.../your_main.py
        pyfiles: Optional[List[str]] = None,  # gs://.../lib1.py, ...
        arguments: Optional[List[str]] = None,
        files: Optional[List[str]] = None,
        archives: Optional[List[str]] = None,
        jars: Optional[List[str]] = None,
        properties: Optional[Dict[str, str]] = None,
        labels: Optional[Dict[str, str]] = None,
        job_name: Optional[str] = None,
    ) -> Dict:
        """
        Returns a Dataproc job dict ready for submission.
        """
        b = DataProcJobBuilder(
            project_id=self.project_id,
            task_id=task_id,
            cluster_name=cluster_name,
            job_type="pyspark_job",
            properties=properties,
        )
        b.set_python_main(main)
        b.add_python_file_uris(pyfiles)
        b.add_args(arguments)
        b.add_file_uris(files)
        b.add_archive_uris(archives)
        b.add_jar_file_uris(jars)
        b.add_labels(labels)

        if job_name:
            b.set_job_name(job_name)

        job = b.build()
        self.log.debug("Built Dataproc job payload: %s", job)
        return job

    # -------------------------
    # Submission & monitoring
    # -------------------------
    def submit_pyspark_job(self, job: Dict) -> str:
        """
        Submits the job dict to Dataproc and returns the Dataproc Job ID.
        """
        self._log_gcloud_equivalent(job)
        resp = self._hook.submit_job(
            job=job,
            project_id=self.project_id,
            region=self.region,
        )
        job_id = resp.reference.job_id  # protobuf-ish object, provider returns Job
        self.log.info("Dataproc job submitted: %s", job_id)
        return job_id

    def wait_for_job(
        self,
        job_id: str,
        *,
        wait_time: int = 10,
        timeout: Optional[int] = None,
    ) -> None:
        """
        Polls until job finishes (DONE/ERROR/CANCELLED) or timeout.
        Raises AirflowException on timeout or API error.
        """
        self.log.info("Waiting for job %s (poll=%ss, timeout=%s)...", job_id, wait_time, timeout)
        self._hook.wait_for_job(
            job_id=job_id,
            project_id=self.project_id,
            region=self.region,
            wait_time=wait_time,
            timeout=timeout,
        )
        self.log.info("Job %s finished.", job_id)

    def get_job(self, job_id: str):
        """
        Returns the Job proto (status, driver output resource URI, etc.).
        """
        return self._hook.get_job(
            job_id=job_id,
            project_id=self.project_id,
            region=self.region,
        )

    # -------------------------
    # Utilities
    # -------------------------
    def _log_gcloud_equivalent(self, job: Dict) -> None:
        """
        Best-effort reconstruction of a faithful gcloud command for your logs.
        Matches what is actually being sent in `job`.
        """
        j = job.get("job", {})
        spec = j.get("pyspark_job", {})
        main = spec.get("main_python_file_uri")
        pyfiles = spec.get("python_file_uris", [])
        files = spec.get("file_uris", [])
        archives = spec.get("archive_uris", [])
        jars = spec.get("jar_file_uris", [])
        args = spec.get("args", [])
        cluster = j.get("placement", {}).get("cluster_name")

        cmd = [
            "gcloud", "dataproc", "jobs", "submit", "pyspark", shlex.quote(main or ""),
            f"--cluster={shlex.quote(cluster or '')}",
            f"--region={shlex.quote(self.region)}",
        ]
        if pyfiles:
            cmd.append(f"--py-files={shlex.quote(','.join(pyfiles))}")
        if files:
            cmd.append(f"--files={shlex.quote(','.join(files))}")
        if archives:
            cmd.append(f"--archives={shlex.quote(','.join(archives))}")
        if jars:
            cmd.append(f"--jars={shlex.quote(','.join(jars))}")
        if args:
            cmd.append("-- " + " ".join(shlex.quote(a) for a in args))

        self.log.info("Equivalent gcloud command:\n%s", " \\\n  ".join(cmd))


# -------------------------
# Example usage inside an Airflow DAG task (PythonOperator or within an Operator)
# -------------------------
def submit_with_service(
    *,
    project_id: str,
    region: str,
    cluster_name: str,
    main: str,
    pyfiles: Optional[List[str]] = None,
    arguments: Optional[List[str]] = None,
    files: Optional[List[str]] = None,
    archives: Optional[List[str]] = None,
    jars: Optional[List[str]] = None,
    labels: Optional[Dict[str, str]] = None,
    gcp_conn_id: str = "google_cloud_default",
    impersonation_chain: Optional[Union[str, Sequence[str]]] = None,
    wait: bool = True,
    wait_time: int = 10,
    timeout: Optional[int] = None,
) -> str:
    """
    Thin function you can call from a DAG. Returns the Dataproc job_id.
    """
    svc = SparkService(
        project_id=project_id,
        region=region,
        gcp_conn_id=gcp_conn_id,
        impersonation_chain=impersonation_chain,
    )

    task_id = f"submit_{datetime.datetime.utcnow().strftime('%Y%m%d_%H%M%S')}"
    job = svc.build_pyspark_job(
        task_id=task_id,
        cluster_name=cluster_name,
        main=main,
        pyfiles=pyfiles,
        arguments=arguments,
        files=files,
        archives=archives,
        jars=jars,
        labels=labels,
        # job_name=None  # optional explicit job name
    )

    job_id = svc.submit_pyspark_job(job)
    if wait:
        svc.wait_for_job(job_id, wait_time=wait_time, timeout=timeout)
    return job_id
