from __future__ import annotations

import time
from typing import Any, Dict, Iterable, Optional

from airflow.models import BaseOperator
from airflow.utils.context import Context

# Airflow GCP provider bits
from airflow.providers.google.cloud.hooks.dataproc import DataprocHook
try:
    # Most recent locations for the builder
    from airflow.providers.google.cloud.utils.dataproc import DataProcJobBuilder
except Exception:
    # Fallback for older layouts (rare with 10.21.1, but keeps this class sturdier)
    from airflow.providers.google.cloud.operators.dataproc import DataProcJobBuilder  # type: ignore


class SparkService(BaseOperator):
    """
    A lean operator that builds & submits a Dataproc job (PySpark by default)
    with:
      - _log_gcloud_cli_equivalent(): log the equivalent gcloud command
      - _wait_for_done(): poll until DONE / ERROR / CANCELLED
      - __get_status(): small helper to read job status safely

    Defaults assume PySpark jobs; you can pass spark_properties / jars / files / archives / python_files, etc.
    """

    template_fields = (
        "project_id",
        "region",
        "cluster_name",
        "main_python_file_uri",
        "args",
        "job_name",
    )

    def __init__(
        self,
        *,
        project_id: str,
        region: str,
        cluster_name: str,
        main_python_file_uri: str,
        args: Optional[Iterable[str]] = None,
        job_name: Optional[str] = None,
        # Optional extras
        gcp_conn_id: str = "google_cloud_default",
        labels: Optional[Dict[str, str]] = None,
        properties: Optional[Dict[str, str]] = None,
        python_files: Optional[Iterable[str]] = None,
        files: Optional[Iterable[str]] = None,
        archives: Optional[Iterable[str]] = None,
        jars: Optional[Iterable[str]] = None,
        service_account: Optional[str] = None,
        wait_check_interval_seconds: int = 20,
        **kwargs: Any,
    ) -> None:
        super().__init__(**kwargs)
        self.project_id = project_id
        self.region = region
        self.cluster_name = cluster_name
        self.main_python_file_uri = main_python_file_uri
        self.args = list(args or [])
        self.job_name = job_name

        self.gcp_conn_id = gcp_conn_id
        self.labels = labels or {}
        self.properties = properties or {}
        self.python_files = list(python_files or [])
        self.files = list(files or [])
        self.archives = list(archives or [])
        self.jars = list(jars or [])
        self.service_account = service_account
        self.wait_check_interval_seconds = max(5, wait_check_interval_seconds)

        # lazily created in execute()
        self._hook: Optional[DataprocHook] = None

    # ----------- Airflow lifecycle -----------

    def execute(self, context: Context) -> Dict[str, Any]:
        """Builds a PySpark job payload, logs gcloud equivalent, submits, then blocks until completion."""
        self._hook = DataprocHook(gcp_conn_id=self.gcp_conn_id)

        # Build the job payload with the DataProcJobBuilder
        builder = DataProcJobBuilder(
            job_name=self.job_name,
            cluster_name=self.cluster_name,
            project_id=self.project_id,
            labels=self.labels,
            region=self.region,
        )

        # Configure it as a PySpark job
        builder.set_pyspark_job(
            main=self.main_python_file_uri,
            args=self.args,
            jars=self.jars or None,
            files=self.files or None,
            archives=self.archives or None,
            python_files=self.python_files or None,
            properties=self.properties or None,
            service_account=self.service_account or None,
        )

        job_payload = builder.build()

        # Log a friendly gcloud CLI equivalent for reproducibility
        self._log_gcloud_cli_equivalent(job_payload)

        # Submit the job
        submitted = self._hook.submit_job(
            project_id=self.project_id,
            job=job_payload,
            region=self.region,
        )

        # The response contains the server-side job reference (dict-like)
        job_id = submitted.get("reference", {}).get("jobId")
        if not job_id:
            # Fallback: sometimes the returned object is a proto; be defensive:
            ref = getattr(submitted, "reference", None)
            job_id = getattr(ref, "job_id", None)

        if not job_id:
            raise RuntimeError("Failed to extract Dataproc jobId from submit response.")

        self.log.info("Dataproc job submitted: %s", job_id)

        # Block until it finishes
        final = self._wait_for_done(job_id)
        state, details = self.__get_status(final)

        # Bubble up failures for task-state correctness
        if state in {"ERROR", "CANCELLED"}:
            raise RuntimeError(f"Dataproc job {job_id} ended in {state}: {details or 'no details'}")

        self.log.info("Dataproc job %s finished with state %s", job_id, state)
        return final

    # ----------- Helpers -----------

    def _log_gcloud_cli_equivalent(self, job: Dict[str, Any]) -> None:
        """
        Emits a 'close-enough' gcloud command that reproduces this job.
        Focuses on PySpark jobs to keep the output compact.
        """
        project = self.project_id
        region = self.region
        cluster = job.get("placement", {}).get("clusterName") or self.cluster_name

        pyspark = job.get("pysparkJob", {})  # type: ignore[assignment]
        main = pyspark.get("mainPythonFileUri") or self.main_python_file_uri
        args = pyspark.get("args", []) or self.args
        jars = pyspark.get("jarFileUris", []) or self.jars
        files = pyspark.get("fileUris", []) or self.files
        archives = pyspark.get("archiveUris", []) or self.archives
        pyfiles = pyspark.get("pythonFileUris", []) or self.python_files
        props = (job.get("job", {}) or {}).get("properties") or job.get("properties") or self.properties

        # Build the CLI with sensible escaping (basic; adjust if you need strict POSIX quoting)
        parts = [
            "gcloud dataproc jobs submit pyspark",
            f"{self._shq(main)}",
            f"--cluster={self._shq(cluster)}",
            f"--region={self._shq(region)}",
            f"--project={self._shq(project)}",
        ]

        for k, v in (props or {}).items():
            parts.append(f"--properties={self._shq(f'{k}={v}')}")
        for j in jars:
            parts.append(f"--jars={self._shq(j)}")
        for f in files:
            parts.append(f"--files={self._shq(f)}")
        for a in archives:
            parts.append(f"--archives={self._shq(a)}")
        for p in pyfiles:
            parts.append(f"--py-files={self._shq(p)}")
        if args:
            parts.append("-- " + " ".join(self._shq(x) for x in args))

        cli = " ".join(parts)
        self.log.info("gcloud equivalent:\n%s", cli)

    def _wait_for_done(self, job_id: str) -> Dict[str, Any]:
        """
        Poll the Dataproc job until it reaches a terminal state.
        Returns the final job dict.
        """
        assert self._hook is not None, "Hook not initialized"
        terminal_states = {"DONE", "ERROR", "CANCELLED"}

        while True:
            job = self._hook.get_job(
                project_id=self.project_id,
                region=self.region,
                job_id=job_id,
            )
            state, details = self.__get_status(job)
            self.log.info("Job %s current state: %s%s", job_id, state, f" ({details})" if details else "")

            if state in terminal_states:
                return job

            time.sleep(self.wait_check_interval_seconds)

    def __get_status(self, job: Dict[str, Any]) -> tuple[str, Optional[str]]:
        """
        Safely extract (state, details) from the job object, working with dicts or proto-like objects.
        """
        # Dict-like path
        try:
            status = job.get("status", {})  # type: ignore[assignment]
            state = status.get("state")
            details = status.get("details")
            if state:
                return str(state), details
        except Exception:
            pass

        # Proto-like path (defensive)
        status = getattr(job, "status", None)
        state = getattr(status, "state", None)
        details = getattr(status, "details", None)
        return (str(state) if state is not None else "UNKNOWN", details)

    # ----------- tiny utils -----------

    @staticmethod
    def _shq(s: Any) -> str:
        """Minimal shell-quoting to make the logged gcloud command safer to copy-paste."""
        text = str(s)
        if not text or any(c.isspace() for c in text) or any(c in text for c in "\"'\\$`"):
            # wrap and escape single quotes by closing/opening; POSIX-lite
            return "'" + text.replace("'", "'\"'\"'") + "'"
        return text
